{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6aTPY0D9V5yJFeVaN72c4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FibGro/Pyspark/blob/main/Skewness_in_DF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3cNDgvn2f48",
        "outputId": "86dbee5d-9fd6-4f08-c331-6a4b5e37d35e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=aee6f63aa9a5dfc0410b76cdd31c026b34e11c2eeba4752b47ea56b78101fb84\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"SkewinDF\").getOrCreate()"
      ],
      "metadata": {
        "id": "xrlSjniA2pfY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Data Skew\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "\n",
        "# sale dataset:\n",
        "# table 1: OrderID, Qty, Sales, Discount (yes=1, no=0)\n",
        "# table 2: ProductID, OrderID, Product, Price\n",
        "\n",
        "########### Table 1 ###########\n",
        "\n",
        "key_1 = [101] * 100\n",
        "key_2 = [201] * 7000000\n",
        "key_3 = [301] * 500\n",
        "key_4 = [401] * 10000\n",
        "OrderID = key_1 + key_2 + key_3 + key_4\n",
        "random.shuffle(OrderID)\n",
        "\n",
        "Qty_1 = list(np.random.randint(low = 1, high = 100, size = len(key_1)))\n",
        "Qty_2 = list(np.random.randint(low = 1, high = 200, size = len(key_2)))\n",
        "Qty_3 = list(np.random.randint(low = 1, high = 1000, size = len(key_3)))\n",
        "Qty_4 = list(np.random.randint(low = 1, high = 50, size = len(key_4)))\n",
        "Qty = Qty_1 + Qty_2 + Qty_3 + Qty_4\n",
        "\n",
        "Sales_1 = list(np.random.randint(low = 10, high = 100, size = len(key_1)))\n",
        "Sales_2 = list(np.random.randint(low = 50, high = 3400, size = len(key_2)))\n",
        "Sales_3 = list(np.random.randint(low = 12, high = 2000, size = len(key_3)))\n",
        "Sales_4 = list(np.random.randint(low = 40, high = 1000, size = len(key_4)))\n",
        "Sales = Sales_1 + Sales_2 + Sales_3 + Sales_4\n",
        "\n",
        "Discount = list(np.random.randint(low = 0, high = 2, size = len(OrderID)))\n",
        "data1 = list(zip(OrderID,Qty,Sales,Discount))\n",
        "\n",
        "# Create the Pandas DF\n",
        "data_skew = pd.DataFrame(data1, columns=['OrderID','Qty','Sales','Discount'])\n",
        "\n",
        "\n",
        "########### Table 2 ###########\n",
        "data2 = [[1, 101, 'pencil', 4.99],\n",
        "         [2, 101, 'book', 9.5],\n",
        "         [3, 101, 'scissors', 14],\n",
        "         [4, 301, 'glue', 7],\n",
        "         [5, 201, 'marker', 8.49],\n",
        "         [6, 301, 'label', 2],\n",
        "         [7, 201, 'calculator', 3.99],\n",
        "         [8, 501, 'eraser', 1.55],\n",
        "        ]\n",
        "\n",
        "data_small = pd.DataFrame(data2, columns=['ProductID', 'OrderID', 'Product', 'Price'])"
      ],
      "metadata": {
        "id": "dJCCpSPj2pnx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create PySpark DF from Pandas\n",
        "\n",
        "# Optimize conversion between PySpark and Pandas DF: Enable arrow-based columnar data transfers\n",
        "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
        "\n",
        "df_skew = spark.createDataFrame(data_skew)\n",
        "df_skew.printSchema()\n",
        "df_skew.show()\n",
        "df_skew.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtBF29-N2pt0",
        "outputId": "72358395-6c8a-4b07-d10c-d35fb6ecbd97"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- OrderID: long (nullable = true)\n",
            " |-- Qty: long (nullable = true)\n",
            " |-- Sales: long (nullable = true)\n",
            " |-- Discount: long (nullable = true)\n",
            "\n",
            "+-------+---+-----+--------+\n",
            "|OrderID|Qty|Sales|Discount|\n",
            "+-------+---+-----+--------+\n",
            "|    201| 70|   38|       1|\n",
            "|    201| 87|   16|       1|\n",
            "|    201| 86|   88|       0|\n",
            "|    201| 25|   90|       0|\n",
            "|    201| 47|   40|       1|\n",
            "|    201| 63|   76|       0|\n",
            "|    201| 46|   39|       0|\n",
            "|    201| 42|   42|       1|\n",
            "|    201| 95|   47|       0|\n",
            "|    201| 58|   77|       1|\n",
            "|    201| 65|   81|       1|\n",
            "|    201| 74|   41|       1|\n",
            "|    201|  5|   42|       1|\n",
            "|    201| 95|   44|       0|\n",
            "|    201| 14|   45|       1|\n",
            "|    201| 17|   62|       1|\n",
            "|    201| 34|   53|       0|\n",
            "|    201| 77|   44|       1|\n",
            "|    201| 16|   91|       0|\n",
            "|    201| 77|   60|       1|\n",
            "+-------+---+-----+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "702"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_small = spark.createDataFrame(data_small)\n",
        "df_small.printSchema()\n",
        "df_small.show()\n",
        "df_small.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nufG21Zy2pw1",
        "outputId": "0c5b5c20-dc77-4be1-8157-43dd66274ec5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- ProductID: long (nullable = true)\n",
            " |-- OrderID: long (nullable = true)\n",
            " |-- Product: string (nullable = true)\n",
            " |-- Price: double (nullable = true)\n",
            "\n",
            "+---------+-------+----------+-----+\n",
            "|ProductID|OrderID|   Product|Price|\n",
            "+---------+-------+----------+-----+\n",
            "|        1|    101|    pencil| 4.99|\n",
            "|        2|    101|      book|  9.5|\n",
            "|        3|    101|  scissors| 14.0|\n",
            "|        4|    301|      glue|  7.0|\n",
            "|        5|    201|    marker| 8.49|\n",
            "|        6|    301|     label|  2.0|\n",
            "|        7|    201|calculator| 3.99|\n",
            "|        8|    501|    eraser| 1.55|\n",
            "+---------+-------+----------+-----+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RUn shuffle JOin() with small sized data"
      ],
      "metadata": {
        "id": "csxm9ke32pzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joined_df = df_skew.join(df_small, df_skew.OrderID == df_small.OrderID, how = \"inner\")\n",
        "# DF increases the parition number to 200 automatically when spark performs data shuffling (join, aggregation)\n",
        "\n"
      ],
      "metadata": {
        "id": "zzW3G0E93JhD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.sql.shuffle.partitions\", 8)\n",
        "\n",
        "# run join()\n",
        "joined_df = df_skew.join(df_small, df_skew.OrderID == df_small.OrderID, how = \"inner\")\n"
      ],
      "metadata": {
        "id": "2zQaxZsI3Jqu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform a join() and descriptive statistics on a skewed data\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "\n",
        "groups = df_skew.join(df_small, df_skew.OrderID == df_small.OrderID, how = \"inner\")\n",
        "\n",
        "summary = groups.select(mean(groups.Sales).alias(\"AVG(Sales)\"),\n",
        "                        stddev(groups.Sales).alias(\"STD(Sales)\"),\n",
        "                        min(groups.Sales).alias(\"MIN(Sales)\"),\n",
        "                        max(groups.Sales).alias(\"MAX(Sales)\"),\n",
        "                       )\n",
        "summary.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnA6OVah3Jtu",
        "outputId": "ddb51e8f-93fb-4712-c974-464494af48dc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----------------+----------+----------+\n",
            "|        AVG(Sales)|       STD(Sales)|MIN(Sales)|MAX(Sales)|\n",
            "+------------------+-----------------+----------+----------+\n",
            "|1722.8482428060252|967.6718604469153|        11|      3399|\n",
            "+------------------+-----------------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mitigate data skewness: SALTING\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# add  random value [0 2]\n",
        "df_skew_salting = df_skew.withColumn(\"_salt_\", round(rand() * 2))\n",
        "df_small_salting = df_small.withColumn(\"_salt_\", round(rand() * 2))\n",
        "\n",
        "df_skew_salting.show()\n",
        "df_small_salting.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MUG3nOt3Jw6",
        "outputId": "cb22d181-242b-41c1-d2ec-67486ccfcf3d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+-----+--------+------+\n",
            "|OrderID|Qty|Sales|Discount|_salt_|\n",
            "+-------+---+-----+--------+------+\n",
            "|    201| 70|   38|       1|   0.0|\n",
            "|    201| 87|   16|       1|   2.0|\n",
            "|    201| 86|   88|       0|   1.0|\n",
            "|    201| 25|   90|       0|   0.0|\n",
            "|    201| 47|   40|       1|   0.0|\n",
            "|    201| 63|   76|       0|   2.0|\n",
            "|    201| 46|   39|       0|   1.0|\n",
            "|    201| 42|   42|       1|   1.0|\n",
            "|    201| 95|   47|       0|   2.0|\n",
            "|    201| 58|   77|       1|   1.0|\n",
            "|    201| 65|   81|       1|   1.0|\n",
            "|    201| 74|   41|       1|   2.0|\n",
            "|    201|  5|   42|       1|   2.0|\n",
            "|    201| 95|   44|       0|   1.0|\n",
            "|    201| 14|   45|       1|   1.0|\n",
            "|    201| 17|   62|       1|   1.0|\n",
            "|    201| 34|   53|       0|   1.0|\n",
            "|    201| 77|   44|       1|   1.0|\n",
            "|    201| 16|   91|       0|   0.0|\n",
            "|    201| 77|   60|       1|   1.0|\n",
            "+-------+---+-----+--------+------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+---------+-------+----------+-----+------+\n",
            "|ProductID|OrderID|   Product|Price|_salt_|\n",
            "+---------+-------+----------+-----+------+\n",
            "|        1|    101|    pencil| 4.99|   1.0|\n",
            "|        2|    101|      book|  9.5|   1.0|\n",
            "|        3|    101|  scissors| 14.0|   0.0|\n",
            "|        4|    301|      glue|  7.0|   1.0|\n",
            "|        5|    201|    marker| 8.49|   1.0|\n",
            "|        6|    301|     label|  2.0|   1.0|\n",
            "|        7|    201|calculator| 3.99|   2.0|\n",
            "|        8|    501|    eraser| 1.55|   1.0|\n",
            "+---------+-------+----------+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# repartition using _salt_:\n",
        "df_skew_salting = df_skew_salting.repartition(3, \"_salt_\")\n",
        "df_small_salting = df_small_salting.repartition(3, \"_salt_\")"
      ],
      "metadata": {
        "id": "HBD3_O6V4uXV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply join()\n",
        "\n",
        "df_skew_salting.drop(\"_salt_\")\n",
        "df_small_salting.drop(\"_salt_\")\n",
        "\n",
        "\n",
        "groups = df_skew_salting.join(df_small_salting, df_skew_salting.OrderID == df_small_salting.OrderID, how = \"inner\")\n",
        "\n",
        "summary = groups.select(mean(groups.Sales).alias(\"AVG(Sales)\"),\n",
        "                        stddev(groups.Sales).alias(\"STD(Sales)\"),\n",
        "                        min(groups.Sales).alias(\"MIN(Sales)\"),\n",
        "                        max(groups.Sales).alias(\"MAX(Sales)\"),\n",
        "                       )\n",
        "summary.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8F9KzeTo4up8",
        "outputId": "9f3768ff-706d-4718-84c8-f35961bed1c0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----------------+----------+----------+\n",
            "|        AVG(Sales)|       STD(Sales)|MIN(Sales)|MAX(Sales)|\n",
            "+------------------+-----------------+----------+----------+\n",
            "|1722.8482428060252|967.6718604468763|        11|      3399|\n",
            "+------------------+-----------------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}